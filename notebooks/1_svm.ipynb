{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3267bd9b",
   "metadata": {},
   "source": [
    "# 1. LP Model for L1-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91168b7a",
   "metadata": {},
   "source": [
    "This is the Julia code for the MATH271 class at Carleton College in Spring 2025 term with Rob Thompson.\n",
    "\n",
    "Final Project: Understanding Breast Cancer Diagnosis and Prognosis via Linear Programming\n",
    "Authors: Palmy Klangsathorn and Angelina Kong\n",
    "Date: 06/05/2025\n",
    "\n",
    "This project explores the application of linear programming techniques to the problem of breast cancer diagnosis and prognosis. We will utilize the Wisconsin Breast Cancer Diagnostic (WBCD) dataset to build and evaluate a predictive model. Our approach will focus on formulating the classification task as a linear program, contrasting it with traditional methods, and analyzing its performance through various statistical metrics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a9635",
   "metadata": {},
   "source": [
    "In this part, we used The L1-SVM, which is a type of Support Vector Machine that we solved using linear programming. It tries to draw a line (or hyperplane) that best separates the two types of tumors. It also uses something called L1 regularization, which helps pick out only the most important features by setting some feature weights to zero. This makes the model simpler and often more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a04f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the data and store it in the dictionary named 'wdbc_data'.\n",
    "\n",
    "# this function read the dataset which is a text file and make a dictionary of the dataset\n",
    "function read_to_dict(file_path::String)\n",
    "  data_dict = Dict{String, Vector{Any}}()\n",
    "  try\n",
    "      open(file_path, \"r\") do io\n",
    "          for line in eachline(io)\n",
    "              # split each element in the line by comma\n",
    "              elements = split(line, ',')\n",
    "              if isempty(elements) || all(isempty, elements)\n",
    "                  continue\n",
    "              end\n",
    "\n",
    "              # the first element is the key and the rest form the list\n",
    "              key = String(strip(elements[1]))\n",
    "\n",
    "              # For 'wdbc.data', the first element is an ID, the second is 'M' or 'B' (diagnosis),\n",
    "              # and the rest are floating-point numbers.\n",
    "              data_list = Vector{Any}()\n",
    "              push!(data_list, String(strip(elements[2]))) # diagnosis 'M' or 'B'\n",
    "              for i in 3:length(elements)\n",
    "                  val_str = strip(elements[i])\n",
    "                  if !isempty(val_str)\n",
    "                      try\n",
    "                          push!(data_list, parse(Float64, val_str))\n",
    "                      catch\n",
    "                          push!(data_list, val_str)\n",
    "                      end\n",
    "                  end\n",
    "              end\n",
    "              data_dict[key] = data_list\n",
    "          end\n",
    "      end\n",
    "  catch e\n",
    "      if isa(e, SystemError) && e.errnum == Base.UV_ENOENT\n",
    "          println(\"Error: File not found\")\n",
    "      else\n",
    "          rethrow(e)\n",
    "      end\n",
    "  end\n",
    "  return data_dict\n",
    "end\n",
    "\n",
    "# our dataset\n",
    "file_path = \"wdbc.data\"\n",
    "wdbc_data = read_to_dict(file_path)\n",
    "println(\"Number of entries in dictionary: $(length(wdbc_data))\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# check check\n",
    "specific_key = \"848406\" \n",
    "if haskey(wdbc_data, specific_key)\n",
    "    println(\"\\nData for key '$specific_key':\")\n",
    "    println(wdbc_data[specific_key])\n",
    "else\n",
    "    println(\"\\nKey '$specific_key' not found in the dictionary.\")\n",
    "end\n",
    "\n",
    "# columns info\n",
    "col_names = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"radius1\", \"texture1\", \"perimeter1\", \"area1\", \"smoothness1\", \"compactness1\", \"concavity1\", \"concave_points1\", \"symmetry1\", \"fractal_dimension1\",\n",
    "    \"radius2\", \"texture2\", \"perimeter2\", \"area2\", \"smoothness2\", \"compactness2\", \"concavity2\", \"concave_points2\", \"symmetry2\", \"fractal_dimension2\",\n",
    "    \"radius3\", \"texture3\", \"perimeter3\", \"area3\", \"smoothness3\", \"compactness3\", \"concavity3\", \"concave_points3\", \"symmetry3\", \"fractal_dimension3\"\n",
    "]\n",
    "\n",
    "print(\"\\nThere are  $(length(col_names)) columns\")\n",
    "print(\"\\nThere are  $(length(col_names)-2) features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Prepocessing\n",
    "\n",
    "using DataFrames\n",
    "\n",
    "# convert all column names to Symbols, which is the standard for DataFrames\n",
    "column_names = Symbol.(col_names)\n",
    "\n",
    "row_tuples = []\n",
    "for (id, values) in wdbc_data\n",
    "    # create a vector for the current row's values\n",
    "    current_row_values = Vector{Any}([id, values[1]]) # start with ID and Diagnosis\n",
    "    append!(current_row_values, values[2:end]) # append all features\n",
    "\n",
    "    # Create a NamedTuple for the current row\n",
    "    # This maps the column names (Symbols) to their respective values\n",
    "    push!(row_tuples, NamedTuple{Tuple(column_names)}(current_row_values))\n",
    "end\n",
    "\n",
    "# Create the DataFrame\n",
    "df = DataFrame(row_tuples)\n",
    "\n",
    "# Now, we can run our subsequent Julia code on 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Encode diagnosis: M = 1, B = -1\n",
    "\n",
    "# The 'Diagnosis' column in the original data contains categorical string labels:\n",
    "# \"M\" for malignant and \"B\" for benign tumors.\n",
    "\n",
    "# Most machine learning algorithms require numeric inputs, not strings. \n",
    "# So we need to convert these string labels into numeric values.\n",
    "\n",
    "# In this step, we use the `ifelse.` broadcasting function to perform element-wise conversion:\n",
    "# - If the diagnosis is \"M\" (malignant), we encode it as 1.0\n",
    "# - If the diagnosis is \"B\" (benign), we encode it as -1.0\n",
    "\n",
    "df.Diagnosis = ifelse.(df.Diagnosis .== \"M\", 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract features and normalize (using StatsBase.zscore)\n",
    "# using Pkg\n",
    "# Pkg.activate(\".\")\n",
    "# Pkg.instantiate()\n",
    "\n",
    "# Pkg.add(\"StatsBase\")\n",
    "using StatsBase \n",
    "using DataFrames\n",
    "\n",
    "# Extract features from 3rd column to end\n",
    "X = Matrix(df[:, 3:end]) \n",
    "y = df.Diagnosis\n",
    "\n",
    "X = zscore(X, 1) # Normalize columns (dims=1 means normalize each column)\n",
    "\n",
    "# Now, X is normalized\n",
    "\n",
    "# By applying zscore(X, 1), we transform each column (feature) of X so that it has a mean of 0 and a standard deviation of 1. \n",
    "# features in the original dataset often have different scales and units (like \"radius\" might be in millimeters, \"area\" in square millimeters,\n",
    "# \"smoothness\" a dimensionless value). Without normalization, features with larger numerical ranges or higher magnitudes would implicitly \n",
    "# contribute more to the distance calculations and objective functions of algorithms. Normalization ensures that all features contribute \n",
    "# proportionally to the model, preventing features with larger scales from dominating the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a48240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm sizes\n",
    "println(\"\\n--- Checking sizes before shuffleobs ---\")\n",
    "println(\"Size of X (rows, cols): \", size(X))\n",
    "println(\"Length of y (rows): \", length(y))\n",
    "println(\"Are X rows and y length equal? \", size(X, 1) == length(y))\n",
    "println(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e819f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train-test split \n",
    "\n",
    "# using Pkg\n",
    "# Pkg.activate(\".\")\n",
    "# Pkg.instantiate()\n",
    "using MLJBase \n",
    "using Random \n",
    "\n",
    "# 'X' (Matrix{Float64}) and 'y' (Vector) are already normalized\n",
    "# get the total number of observations\n",
    "num_observations = size(X, 1)\n",
    "\n",
    "# create a vector of indices\n",
    "indices = collect(1:num_observations)\n",
    "\n",
    "# shuffle the indices in-place\n",
    "Random.shuffle!(indices)\n",
    "\n",
    "# determine split point\n",
    "split_point = floor(Int, num_observations * 0.8)\n",
    "\n",
    "# split the shuffled indices into train and test sets\n",
    "train_indices = indices[1:split_point]\n",
    "test_indices = indices[split_point+1:end]\n",
    "\n",
    "# use these shuffled indices to create your train and test sets\n",
    "X_train = X[train_indices, :]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_test = X[test_indices, :]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "\n",
    "println(\"X_train dimensions: \", size(X_train))\n",
    "println(\"y_train length: \", length(y_train))\n",
    "println(\"X_test dimensions: \", size(X_test))\n",
    "println(\"y_test length: \", length(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be87c6d",
   "metadata": {},
   "source": [
    "# 1.  LP Classifier (L1-SVM) using JuMP\n",
    "\n",
    "We begin with an L1-regularized. We do this by using an L1-regularized Support Vector Machine (L1-SVM), which we reformulate as a linear programming (LP) problem. This LP-based approach helps us make predictions and also makes the model easier to understand, since it tends to focus only on the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: LP Classifier (L1-SVM) using JuMP\n",
    "\n",
    "import Pkg\n",
    "Pkg.add(\"GLPK\")\n",
    "using JuMP\n",
    "using GLPK \n",
    "using LinearAlgebra \n",
    "using Statistics\n",
    "\n",
    "# since X_train, y_train, X_test, y_test are already defined and are numerical (e.g., 1.0 and -1.0)\n",
    "y_train_numeric = y_train\n",
    "y_test_numeric = y_test\n",
    "\n",
    "# Use these numeric labels for training and prediction\n",
    "n, d = size(X_train) # n is now the number of training samples\n",
    "\n",
    "# Define the penalty parameter C for the slack variables.\n",
    "# This is a hyperparameter you might tune. A common starting point is 1.0.\n",
    "C_penalty = 1.0\n",
    "\n",
    "# Create the LP Model\n",
    "model_lp = JuMP.Model(GLPK.Optimizer)\n",
    "\n",
    "@variable(model_lp, w_plus[1:d] >= 0)  # w_j_plus >= 0\n",
    "@variable(model_lp, w_minus[1:d] >= 0) # w_j_minus >= 0\n",
    "@variable(model_lp, b)\n",
    "@variable(model_lp, 両[1:n] >= 0)\n",
    "\n",
    "# Reconstruct 'w' from w_plus and w_minus for use in constraints\n",
    "# Although not explicitly a variable, we define this as an expression for clarity\n",
    "# and use it in the dot product.\n",
    "# JuMP will handle this correctly when building the constraint matrix.\n",
    "# For each j, w[j] = w_plus[j] - w_minus[j]\n",
    "\n",
    "# Linear Objective is to Minimize L1-norm of w + C * sum of slack variables\n",
    "@objective(model_lp, Min, sum(w_plus) + sum(w_minus) + C_penalty * sum(両))\n",
    "\n",
    "# Linear Constraints\n",
    "# The main SVM constraint: y_i * (dot(w, x_i) + b) >= 1 - 両_i\n",
    "# We need to explicitly expand dot(w, X_train[i, :]) using w_plus and w_minus\n",
    "@constraint(model_lp, [i=1:n],\n",
    "    y_train_numeric[i] * (\n",
    "        sum(X_train[i, j] * (w_plus[j] - w_minus[j]) for j in 1:d) + b\n",
    "    ) >= 1 - 両[i]\n",
    ")\n",
    "\n",
    "# Optimizing\n",
    "optimize!(model_lp)\n",
    "\n",
    "# Check if the optimization was successful\n",
    "if termination_status(model_lp) == MOI.OPTIMAL\n",
    "    # Recover optimal w and b\n",
    "    w_opt_lp = value.(w_plus) .- value.(w_minus) # w_j = w_j_plus - w_j_minus\n",
    "    b_opt_lp = value(b)\n",
    "\n",
    "    # Step 7: Predict and Evaluate for LP SVM\n",
    "    function predict_lp(X, w_lp, b_lp)\n",
    "        return sign.(X * w_lp .+ b_lp)\n",
    "    end\n",
    "\n",
    "    y_pred_lp = predict_lp(X_test, w_opt_lp, b_opt_lp)\n",
    "\n",
    "    # evaluation metrics\n",
    "    TP_lp = sum((y_test_numeric .== 1) .&& (y_pred_lp .== 1))\n",
    "    TN_lp = sum((y_test_numeric .== -1) .&& (y_pred_lp .== -1))\n",
    "    FP_lp = sum((y_test_numeric .== -1) .&& (y_pred_lp .== 1))\n",
    "    FN_lp = sum((y_test_numeric .== 1) .&& (y_pred_lp .== -1))\n",
    "\n",
    "    println(\"\\n--- Confusion Matrix (LP SVM) ---\")\n",
    "    println(\"True Positives (TP):  \", TP_lp)\n",
    "    println(\"True Negatives (TN):  \", TN_lp)\n",
    "    println(\"False Positives (FP): \", FP_lp)\n",
    "    println(\"False Negatives (FN): \", FN_lp)\n",
    "\n",
    "    accuracy_lp = (TP_lp + TN_lp) / (TP_lp + TN_lp + FP_lp + FN_lp)\n",
    "    println(\"\\nTest accuracy (LP SVM): \", round(accuracy_lp * 100, digits=2), \"%\")\n",
    "\n",
    "    precision_lp = TP_lp / (TP_lp + FP_lp)\n",
    "    println(\"Precision (LP SVM): \", round(precision_lp * 100, digits=2), \"%\")\n",
    "\n",
    "    recall_lp = TP_lp / (TP_lp + FN_lp)\n",
    "    println(\"Recall (Sensitivity) (LP SVM): \", round(recall_lp * 100, digits=2), \"%\")\n",
    "\n",
    "    f1_score_lp = 2 * (precision_lp * recall_lp) / (precision_lp + recall_lp)\n",
    "    println(\"F1-Score (LP SVM): \", round(f1_score_lp * 100, digits=2), \"%\")\n",
    "\n",
    "    specificity_lp = TN_lp / (TN_lp + FP_lp)\n",
    "    println(\"Specificity (LP SVM): \", round(specificity_lp * 100, digits=2), \"%\")\n",
    "\n",
    "else\n",
    "    println(\"LP Optimization failed with status: \", termination_status(model_lp))\n",
    "    println(\"Primal Status: \", primal_status(model_lp))\n",
    "    println(\"Dual Status: \", dual_status(model_lp))\n",
    "    w_opt_lp = nothing\n",
    "    b_opt_lp = nothing\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MultivariateStats\n",
    "using Plots\n",
    "# using MathOptInterface as MOI # Uncomment if MathOptInterface is needed \n",
    "# specifically in this cell for `termination_status`\n",
    "\n",
    "# Check Optimization Status and Plotting Logic for Decision Boundary or PCA \n",
    "if termination_status(model_lp) == MOI.OPTIMAL\n",
    "    if d == 2\n",
    "        println(\"\\n--- Generating 2D Decision Boundary Plot ---\")\n",
    "        x_min, x_max = extrema(X_train[:, 1])\n",
    "        y_min, y_max = extrema(X_train[:, 2])\n",
    "\n",
    "        x_buffer = (x_max - x_min) * 0.1\n",
    "        y_buffer = (y_max - y_min) * 0.1\n",
    "        plot_x_min, plot_x_max = x_min - x_buffer, x_max + x_buffer\n",
    "        plot_y_min, plot_y_max = y_min - y_buffer, y_max + y_buffer\n",
    "\n",
    "        num_grid_points = 100\n",
    "        test_range_x = range(plot_x_min, stop=plot_x_max, length=num_grid_points)\n",
    "        test_range_y = range(plot_y_min, stop=plot_y_max, length=num_grid_points)\n",
    "\n",
    "        Z = Matrix{Float64}(undef, num_grid_points, num_grid_points)\n",
    "\n",
    "        for (i, x_val) in enumerate(test_range_x)\n",
    "            for (j, y_val) in enumerate(test_range_y)\n",
    "                grid_point = [x_val y_val]\n",
    "                # Ensure predict_lp handles a 1x2 matrix correctly,\n",
    "                # or reshape grid_point if predict_lp expects a vector.\n",
    "                Z[i, j] = predict_lp(grid_point, w_opt_lp, b_opt_lp)[1]\n",
    "            end\n",
    "        end\n",
    "\n",
    "        p = plot(;\n",
    "            xlim=(plot_x_min, plot_x_max),\n",
    "            ylim=(plot_y_min, plot_y_max),\n",
    "            aspect_ratio=1,\n",
    "            title=\"L1-SVM Decision Boundary\",\n",
    "            xlabel=\"Feature 1\",\n",
    "            ylabel=\"Feature 2\",\n",
    "            legend=:outertopright,\n",
    "            left_margin=5Plots.mm \n",
    "        )\n",
    "\n",
    "        contourf!(\n",
    "            p,\n",
    "            test_range_x,\n",
    "            test_range_y,\n",
    "            Z;\n",
    "            levels=1,\n",
    "            color=cgrad(:redsblues),\n",
    "            alpha=0.7,\n",
    "            colorbar_title=\"Predicted Class\",\n",
    "            label=\"\",\n",
    "        )\n",
    "\n",
    "        X1 = X_train[y_train_numeric .== -1, :]\n",
    "        X2 = X_train[y_train_numeric .== 1, :]\n",
    "\n",
    "        scatter!(p, X1[:, 1], X1[:, 2]; color=:red, label=\"Class -1\", markershape=:circle, markersize=5)\n",
    "        scatter!(p, X2[:, 1], X2[:, 2]; color=:blue, label=\"Class 1\", markershape=:xcross, markersize=5)\n",
    "\n",
    "        display(p)\n",
    "        # savefig(p, \"svm_decision_boundary_2d.png\") # Uncomment to save\n",
    "\n",
    "    else # d > 2, so proceed with PCA visualization\n",
    "        println(\"\\n--- Attempting 2D Visualization via PCA ---\")\n",
    "\n",
    "        # Ensure X_train is Float64 and correctly oriented (features as rows for MultivariateStats.jl)\n",
    "        X_train_for_pca = permutedims(X_train) # Transpose X_train to be (features x samples)\n",
    "\n",
    "        # Corrected fit call:\n",
    "        M = MultivariateStats.fit(MultivariateStats.PCA, X_train_for_pca; maxoutdim=2)\n",
    "\n",
    "        # Corrected transform call:\n",
    "        X_train_pca = MultivariateStats.transform(M, X_train_for_pca) # This will be 2 x n_samples\n",
    "\n",
    "        # Now, for plotting, we need (n_samples x 2)\n",
    "        X_train_pca_plot = permutedims(X_train_pca)\n",
    "\n",
    "        # 3. Determine plot limits from transformed data\n",
    "        x_min_pca, x_max_pca = extrema(X_train_pca_plot[:, 1])\n",
    "        y_min_pca, y_max_pca = extrema(X_train_pca_plot[:, 2])\n",
    "\n",
    "        x_buffer_pca = (x_max_pca - x_min_pca) * 0.1\n",
    "        y_buffer_pca = (y_max_pca - y_min_pca) * 0.1\n",
    "        plot_x_min_pca, plot_x_max_pca = x_min_pca - x_buffer_pca, x_max_pca + x_buffer_pca\n",
    "        plot_y_min_pca, plot_y_max_pca = y_min_pca - y_buffer_pca, y_max_pca + y_buffer_pca\n",
    "\n",
    "        p_pca = plot(;\n",
    "            xlim=(plot_x_min_pca, plot_x_max_pca),\n",
    "            ylim=(plot_y_min_pca, plot_y_max_pca),\n",
    "            aspect_ratio=1,\n",
    "            title=\"L1-SVM Data in PCA Space (d=$(d) reduced to 2)\",\n",
    "            xlabel=\"Principal Component 1\",\n",
    "            ylabel=\"Principal Component 2\",\n",
    "            legend=:outertopright,\n",
    "            left_margin=5Plots.mm # <-- Added this line to fix y-axis label cutoff\n",
    "        )\n",
    "        \n",
    "        # Scatter plot for transformed training data\n",
    "        X1_pca = X_train_pca_plot[y_train_numeric .== -1, :]\n",
    "        X2_pca = X_train_pca_plot[y_train_numeric .== 1, :]\n",
    "\n",
    "        scatter!(p_pca, X1_pca[:, 1], X1_pca[:, 2]; color=:red, label=\"Class Benign\", markershape=:circle, markersize=5)\n",
    "        scatter!(p_pca, X2_pca[:, 1], X2_pca[:, 2]; color=:blue, label=\"Class Malignant\", markershape=:xcross, markersize=5)\n",
    "\n",
    "        display(p_pca)\n",
    "        savefig(p_pca, \"images/svm_pca_plot.png\") \n",
    "    end\n",
    "else\n",
    "    println(\"LP Optimization failed. No visualization for decision boundary or PCA was generated.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using Printf \n",
    "\n",
    "# Plotting Feature Coefficients (runs only if optimization was successful and d > 2)\n",
    "if termination_status(model_lp) == MOI.OPTIMAL && d > 2\n",
    "    \n",
    "    feature_names = col_names[3:end]\n",
    "\n",
    "    println(\"\\n--- Plotting Feature Coefficients ---\")\n",
    "    feature_indices = 1:d \n",
    "\n",
    "    annotations = []\n",
    "    \n",
    "    # Determine the maximum absolute coefficient for scaling\n",
    "    max_abs_coeff = isempty(w_opt_lp) ? 0.0 : maximum(abs.(w_opt_lp))\n",
    "\n",
    "    # Define annotation font size and margin\n",
    "    annotation_fontsize = 5 # You can adjust this value (e.g., 6, 8, 9)\n",
    "    \n",
    "    # Margin for text placement. This is a small value relative to the bar height.\n",
    "    # For text INSIDE, a percentage of the bar's own height (e.g., 80%) or a fixed small offset works.\n",
    "    # For text OUTSIDE (for very short bars), a percentage of the max_abs_coeff works.\n",
    "    outside_margin = max_abs_coeff * 0.05 # 5% of the largest absolute coefficient value for outside placement\n",
    "    inside_margin_factor = 0.8 # Place text at 80% of the bar's height (from base)\n",
    "    \n",
    "    # Define a threshold below which text might not fit well inside the bar\n",
    "    # This might need fine-tuning based on your specific plot range and font size\n",
    "    min_coeff_for_inside_text = max_abs_coeff * 0.1 # If bar height is less than 10% of max, place text outside\n",
    "\n",
    "    # If all coefficients are zero, set a small default for positioning\n",
    "    if iszero(max_abs_coeff)\n",
    "        outside_margin = 0.05 \n",
    "        min_coeff_for_inside_text = 0.01\n",
    "    end\n",
    "\n",
    "    for i in 1:length(w_opt_lp)\n",
    "        coeff_value = w_opt_lp[i]\n",
    "        text_label = @sprintf \"%.2f\" coeff_value\n",
    "        \n",
    "        y_pos = 0.0 \n",
    "\n",
    "        # Decide whether to place text inside or outside the bar\n",
    "        if abs(coeff_value) > min_coeff_for_inside_text\n",
    "            # Place text INSIDE the bar\n",
    "            if coeff_value > 0\n",
    "                y_pos = coeff_value * inside_margin_factor # 80% up from the base for positive bars\n",
    "            else # coeff_value < 0\n",
    "                y_pos = coeff_value * inside_margin_factor # 80% down from the base for negative bars\n",
    "            end\n",
    "        else\n",
    "            # Place text OUTSIDE the bar (for very short or zero bars)\n",
    "            if coeff_value > 0\n",
    "                y_pos = coeff_value + outside_margin\n",
    "            elseif coeff_value < 0\n",
    "                y_pos = coeff_value - outside_margin\n",
    "            else # coeff_value == 0\n",
    "                y_pos = outside_margin # Place slightly above 0 for zero bars\n",
    "            end\n",
    "        end\n",
    "\n",
    "        push!(annotations, (i, y_pos, Plots.text(text_label, annotation_fontsize)))\n",
    "    end\n",
    "        \n",
    "    p_coeffs = bar(feature_indices, w_opt_lp,\n",
    "                   title=\"L1-SVM Feature Coefficients (L1-Norm)\",\n",
    "                   xlabel=\"Feature Name\",\n",
    "                   ylabel=\"Coefficient Value\",\n",
    "                   xticks=(1:length(feature_names), feature_names),\n",
    "                   xrotation=90,\n",
    "                   legend=false,\n",
    "                   size=(1200, 700),\n",
    "                   bottom_margin=15Plots.mm,\n",
    "                   left_margin=10Plots.mm,\n",
    "                   annotations=annotations \n",
    "    )\n",
    "    display(p_coeffs)\n",
    "    savefig(p_coeffs, \"images/svm_plane_coeffs.png\")\n",
    "else\n",
    "    if termination_status(model_lp) != MOI.OPTIMAL\n",
    "        println(\"LP Optimization failed. Cannot plot feature coefficients.\")\n",
    "    elseif d <= 2\n",
    "        println(\"Skipping feature coefficient plot as d <= 2 (decision boundary was plotted).\")\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
